# -*- coding: utf-8 -*-
"""social media.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w3BHG2R1w5XTNhOBYmFu9L0grNdcCTaG
"""

import tweepy

# Set up Twitter API credentials (You need to replace these with your own)
api_key = 'your_api_key'
api_secret_key = 'your_api_secret_key'
access_token = 'your_access_token'
access_token_secret = 'your_access_token_secret'

# Authenticate with Twitter API
auth = tweepy.OAuthHandler(api_key, api_secret_key)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit=True)

# Fetch tweets about a specific topic
def fetch_tweets(topic, count=100):
    tweets = tweepy.Cursor(api.search_tweets, q=topic, lang="en", tweet_mode='extended').items(count)
    tweet_list = [tweet.full_text for tweet in tweets]
    return tweet_list

tweets = fetch_tweets('AI', 200)  # Fetch 200 tweets about AI

import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

# Preprocess tweet text
def preprocess_tweet(text):
    # Remove URLs and special characters
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#', '', text)
    # Tokenize and remove stopwords
    tokens = nltk.word_tokenize(text)
    tokens = [word for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]
    return ' '.join(filtered_tokens)

cleaned_tweets = [preprocess_tweet(tweet) for tweet in tweets]

from textblob import TextBlob

# Analyze sentiment of tweets
def analyze_sentiment(tweet):
    analysis = TextBlob(tweet)
    # Polarity ranges from -1 (negative) to 1 (positive)
    return analysis.sentiment.polarity

sentiment_scores = [analyze_sentiment(tweet) for tweet in cleaned_tweets]

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create a DataFrame for plotting
df = pd.DataFrame({'Tweet': cleaned_tweets, 'Sentiment': sentiment_scores})

# Plot the distribution of sentiment scores
plt.figure(figsize=(10,6))
sns.histplot(df['Sentiment'], bins=20, kde=True)
plt.title('Sentiment Analysis of Tweets')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

import datetime

# Add timestamp information to tweets
def fetch_tweets_with_time(topic, count=100):
    tweets = tweepy.Cursor(api.search_tweets, q=topic, lang="en", tweet_mode='extended').items(count)
    tweet_data = [(tweet.full_text, tweet.created_at) for tweet in tweets]
    return tweet_data

tweets_with_time = fetch_tweets_with_time('AI', 200)

# Preprocess and extract sentiment
tweets_data = [(preprocess_tweet(tweet), time) for tweet, time in tweets_with_time]
sentiment_scores_time = [(analyze_sentiment(tweet), time) for tweet, time in tweets_data]

# Convert to DataFrame
df_time = pd.DataFrame(sentiment_scores_time, columns=['Sentiment', 'Timestamp'])

# Plot sentiment over time
df_time['Timestamp'] = pd.to_datetime(df_time['Timestamp'])
df_time = df_time.sort_values('Timestamp')

plt.figure(figsize=(12,6))
plt.plot(df_time['Timestamp'], df_time['Sentiment'])
plt.title('Sentiment Trend Over Time')
plt.xlabel('Time')
plt.ylabel('Sentiment Score')
plt.xticks(rotation=45)
plt.show()